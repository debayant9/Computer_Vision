{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.25.2\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(np.__version__)\n",
    "\n",
    "def imgshow(title, image = None, size =6):\n",
    "    w,h = image.shape[0], image.shape[1]\n",
    "    aspect_ratio = w/h\n",
    "    plt.figure(figsize=(size*aspect_ratio, size))\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers are needed to cast the image data into the required format for input into our model\n",
    "# Convert to tensors and normalize\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,),(0.5,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch MNIST dataset using torchvision for training and test\n",
    "trainset = torchvision.datasets.MNIST('mnist',\n",
    "                                      train = True, \n",
    "                                      download = True,\n",
    "                                      transform = transform)\n",
    "testset = torchvision.datasets.MNIST('mnist',\n",
    "                                     train=False,\n",
    "                                     download=True,\n",
    "                                     transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(trainset.data.shape)\n",
    "print(testset.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\indtal\\anaconda3\\lib\\site-packages (1.23.1)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.25.1-cp39-cp39-win_amd64.whl (15.1 MB)\n",
      "     -------------------------------------- 15.1/15.1 MB 570.2 kB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.1\n",
      "    Uninstalling numpy-1.23.1:\n",
      "      Successfully uninstalled numpy-1.23.1\n",
      "Successfully installed numpy-1.25.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.3.0 requires daal==2021.2.3, which is not installed.\n",
      "scipy 1.7.1 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.25.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to plot the image in opencv, we need to convert the image to numpy array\n",
    "image = trainset.data[0].numpy()\n",
    "imgshow(\"mnist\",image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dataloader is a function that we will use to grab our data in specified batch sizes (we'l use 128) during training\n",
    "# Prepare train and test loader\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size = 128,\n",
    "                                          shuffle = True,\n",
    "                                          num_workers = 0)\n",
    "testloader = torch.utils.data.DataLoader(testset,\n",
    "                                         batch_size = 128,\n",
    "                                         shuffle = False,\n",
    "                                         num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Iter and Next() for load batches\n",
    "dataiter = iter(trainloader)\n",
    "\n",
    "labels = next(dataiter)\n",
    "\n",
    "#print(images.shape)\n",
    "#print(labels.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a Convolution Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Create model using a Python class\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,32,3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(64 * 12 * 12, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64*12*12)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "# create an instance of the model and move memory and operations to CUDA device\n",
    "net = Net()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a loss function and an optimizer\n",
    "\n",
    "import torch.optim as optim\n",
    "# using cross entropy as the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch: 1...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 128) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39m# Move our data to GPU\u001b[39;00m\n\u001b[0;32m     18\u001b[0m inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 19\u001b[0m labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(labels)\n\u001b[0;32m     20\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(labels\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mlong\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     21\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 128) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "#create some empty arrays to store\n",
    "epoch_log = []\n",
    "loss_log = []\n",
    "accuracy_log = []\n",
    "\n",
    "# Iterate for a specified number of epochs\n",
    "for epoch in range(epochs):\n",
    "    print(f'Starting Epoch: {epoch+1}...')\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    #We iterate through our trainloader iterator\n",
    "    # each cycle is a mini batch\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, data = data\n",
    "        # Move our data to GPU\n",
    "        inputs = inputs.to(device)\n",
    "        labels = np.asarray(labels)\n",
    "        labels = torch.from_numpy(labels.astype('long'))\n",
    "        labels = labels.to(device)\n",
    "        # clear the gradients before training starts\n",
    "        optimizer.zero_grad()\n",
    "        #Forward -> backprop + optimize\n",
    "        outputs = net(inputs) #Forward Propagation \n",
    "        loss = criterion(outputs, labels) # Get Loss (quantify the difference between the results and predictions)\n",
    "        loss.backward() # Back propagate to obtain the new gradients for all nodes\n",
    "        optimizer.step() # Update the gradients/weights\n",
    "\n",
    "        # Print training stats - Epochs/Iterations/Loss/Accuracy\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49: # show loss after 50 iterations\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # torch.no_grad() disables backpropagation and gradient calcualtion which we dont need during test\n",
    "            with torch.no_grad():\n",
    "                for data in testloader:\n",
    "                    images, labels = data\n",
    "                    # Move our data to GPU\n",
    "                    images = images.to(device)\n",
    "                    labels = np.asarray(labels)\n",
    "                    labels = torch.from_numpy(labels.astype('long'))\n",
    "                    labels = labels.to(device)\n",
    "                    outputs = net(images)\n",
    "                    # Get predictions from the maximum value of the predicted output tensor\n",
    "                     # we set dim = 1 as it specifies the number of dimensions to reduce\n",
    "                    _, predicted = torch.max(outputs.data, dim =1)\n",
    "                    total += labels.size(0)\n",
    "                    # Keep a running total of the number of predictions predicted correctly\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                accuracy = 100 * correct / total\n",
    "                epoch_num = epoch + 1\n",
    "                actual_loss = running_loss / 50\n",
    "                print(f'Epoch: {epoch_num}, Mini-Batches Completed: {(i+1)}, Loss: {actual_loss:.3f}, Test Accuracy = {accuracy:.3f}%')\n",
    "                running_loss = 0.0\n",
    "\n",
    "    # Store training stats after each epoch\n",
    "    epoch_log.append(epoch_num)\n",
    "    loss_log.append(actual_loss)\n",
    "    accuracy_log.append(accuracy)\n",
    "\n",
    "print(\"finished training\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our model\n",
    "torch.save(net.state_dict(), \".\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
